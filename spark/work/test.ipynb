{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_data = pd.read_csv(\"./raw_data/sanfrancisco.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_columns = list(sf_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "unused_columns = ['snow_1h', 'snow_24h', 'rain_24h', 'rain_1h', 'rain_today', 'snow_today', 'weather_icon', 'weather_id', 'condition_id', 'sea_level', 'grnd_level', 'lat', 'lon', 'city_id', 'city_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_columns = columns = list(set(raw_columns) - set(unused_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup(raw_data, used_columns):\n",
    "    data = raw_data.loc[:, used_columns] \n",
    "    print(\"fill_na\")\n",
    "    data['rain_3h'] = data['rain_3h'].fillna(0)\n",
    "    data['snow_3h'] = data['snow_3h'].fillna(0)\n",
    "    print(\"drop_duplicates\")\n",
    "    data.drop_duplicates('dt', inplace=True)\n",
    "    print(\"add_new_dada\")\n",
    "    data = add_new_data(data)\n",
    "    \n",
    "    data = data.apply(transform_datetime, axis=1)\n",
    "    \n",
    "    unused_columns  = ['dt_iso', 'condition', 'condition_details', 'dt_datetime']\n",
    "    \n",
    "    data = data.drop(unused_columns, axis=1)\n",
    "    data = data.reset_index(drop=True)\n",
    "    return data\n",
    "\n",
    "def add_new_data(data):\n",
    "    data['dt_datetime'] =  pd.to_datetime(data['dt_iso'], format='%Y-%m-%d %H:%M:%S +%f %Z')\n",
    "    weather_description_columns = list(set(data['condition_details']))\n",
    "    weather_main_columns = list(set(data['condition']))\n",
    "    data = transform_categorical_data(data, weather_description_columns,weather_main_columns)\n",
    "\n",
    "    return data\n",
    "\n",
    "def transform_categorical_data(data, weather_description_columns, weather_main_columns):\n",
    "    for column in weather_description_columns:\n",
    "        data[column] = data['condition_details'] == column\n",
    "        data[column] = data[column].astype(int)\n",
    "        \n",
    "    for column in weather_main_columns:\n",
    "        data[column] = data['condition'] == column\n",
    "        data[column] = data[column].astype(int)\n",
    "    return data\n",
    "    \n",
    "def transform_datetime(current_data):\n",
    "    for month in range(1, 12):\n",
    "        current_data['month_{}'.format(month)] = 1 if current_data['dt_datetime'].month == month else 0\n",
    "\n",
    "    current_data['year'] =  current_data['dt_datetime'].year\n",
    "    current_data['dayofweek'] = current_data['dt_datetime'].dayofweek\n",
    "    current_data['dayofyear'] = current_data['dt_datetime'].dayofyear\n",
    "    current_data['hourofday'] = current_data['dt_datetime'].hour\n",
    "    return current_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fill_na\n",
      "drop_duplicates\n",
      "add_new_dada\n"
     ]
    }
   ],
   "source": [
    "sf_data2 = cleanup(sf_data[1:], used_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_target_data(data):\n",
    "    data['target_temperature'] = data['temperature'][1:].append(pd.Series([np.nan]) , ignore_index=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_data3 = add_target_data(sf_data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def add_previous_datum(raw_data):\n",
    "#     data = raw_data.copy()\n",
    "#     diff_columns = list(set(raw_data.columns) - set(['dt', 'dt_iso', 'dt_datetime']))\n",
    "#     maximum_prev = 2 *  24\n",
    "#     data = data.apply(add_previous_data, args=(diff_columns, data, maximum_prev), axis=1)\n",
    "#     return data\n",
    "\n",
    "# def add_previous_data(current_data,  diff_columns, raw_data, maximum_prev):\n",
    "#     index = current_data.name\n",
    "#     print(index)\n",
    "#     if index == 0:\n",
    "#         return current_data\n",
    "#     prev_right = index - 1\n",
    "#     prev_left = index - maximum_prev if index - maximum_prev > 0 else 0\n",
    "       \n",
    "#     while prev_left <= prev_right:\n",
    "#         current_data = add_diff_data(current_data, raw_data.iloc[prev_left], maximum_prev, diff_columns)\n",
    "#         prev_left += 1\n",
    "#     return current_data\n",
    "\n",
    "# def add_diff_data(current_data, prev_data, maximum_prev, diff_columns):\n",
    "#     diff = int(pd.Timedelta(current_data['dt_datetime'] - prev_data['dt_datetime']).seconds/ 3600)\n",
    "#     if diff > 0 and diff < maximum_prev:\n",
    "#         for diff_column in diff_columns:\n",
    "#             column_name = '{}_{}_ago'.format(diff_column, diff)\n",
    "#             current_data[column_name] = prev_data[diff_column]\n",
    "#     return current_data\n",
    "\n",
    "# # add new data by merging np array and adding dummy data \n",
    "# #[NALL, NALL, data1, data2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "hour_diffs =  [1,2,3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36, 39, 42, 45, 48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_diff_data(raw_data):\n",
    "    data = raw_data.copy()\n",
    "    diff_columns = list(set(raw_data.columns) - set(['dt', 'dt_iso', 'dt_datetime', 'target_temperature', 'month_1', 'month_2', 'month_3', 'month_4', 'month_5', 'month_6', 'month_7', 'month_8', 'month_9', 'month_10', 'month_11', 'month_12', 'year', 'dayofyear', 'dayofweek', 'hourofday', 'temperature_min', 'temperature_max'] + weather_description_columns))\n",
    "    maximum_prev = 2 *  24\n",
    "    for i in hour_diffs:\n",
    "        for column in diff_columns:\n",
    "            data['{}_{}_ago'.format(column, i)] = pd.Series(np.repeat(np.nan, i)).append(data[column][:-i] , ignore_index=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_data4 = add_diff_data(sf_data3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_diff_null_data(raw_data):\n",
    "    data = raw_data[48:-1]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_data5 = remove_diff_null_data(sf_data4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "922    284.82\n",
       "923    283.65\n",
       "924    282.17\n",
       "925    286.47\n",
       "926    286.47\n",
       "Name: target_temperature, dtype: float64"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sf_data5['target_temperature'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2012    879\n",
       "Name: year, dtype: int64"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sf_data5['year'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_data = sf_data5.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target_temperature    1.000000\n",
       "temperature           0.953572\n",
       "temperature_min       0.893503\n",
       "temperature_1_ago     0.867928\n",
       "temperature_max       0.842872\n",
       "temperature_2_ago     0.754620\n",
       "temperature_21_ago    0.730963\n",
       "temperature_24_ago    0.705506\n",
       "temperature_3_ago     0.618969\n",
       "temperature_45_ago    0.517360\n",
       "temperature_18_ago    0.465166\n",
       "temperature_48_ago    0.427962\n",
       "temperature_27_ago    0.388665\n",
       "temperature_42_ago    0.352472\n",
       "hourofday             0.345891\n",
       "humidity_9_ago        0.344411\n",
       "humidity_12_ago       0.304721\n",
       "Clear_12_ago          0.288914\n",
       "Clear_15_ago          0.263123\n",
       "humidity_33_ago       0.245208\n",
       "wind_speed_42_ago     0.244765\n",
       "wind_degree_42_ago    0.240296\n",
       "wind_speed_21_ago     0.240253\n",
       "Clear_9_ago           0.237604\n",
       "wind_speed_45_ago     0.229842\n",
       "wind_speed_18_ago     0.220732\n",
       "temperature_6_ago     0.206740\n",
       "Clear_36_ago          0.191657\n",
       "wind_degree_18_ago    0.188099\n",
       "humidity_6_ago        0.187106\n",
       "                        ...   \n",
       "snow_3h_6_ago              NaN\n",
       "rain_3h_6_ago              NaN\n",
       "snow_3h_9_ago              NaN\n",
       "rain_3h_9_ago              NaN\n",
       "snow_3h_12_ago             NaN\n",
       "rain_3h_12_ago             NaN\n",
       "snow_3h_15_ago             NaN\n",
       "rain_3h_15_ago             NaN\n",
       "snow_3h_18_ago             NaN\n",
       "rain_3h_18_ago             NaN\n",
       "snow_3h_21_ago             NaN\n",
       "rain_3h_21_ago             NaN\n",
       "snow_3h_24_ago             NaN\n",
       "rain_3h_24_ago             NaN\n",
       "snow_3h_27_ago             NaN\n",
       "rain_3h_27_ago             NaN\n",
       "snow_3h_30_ago             NaN\n",
       "rain_3h_30_ago             NaN\n",
       "snow_3h_33_ago             NaN\n",
       "rain_3h_33_ago             NaN\n",
       "snow_3h_36_ago             NaN\n",
       "rain_3h_36_ago             NaN\n",
       "snow_3h_39_ago             NaN\n",
       "rain_3h_39_ago             NaN\n",
       "snow_3h_42_ago             NaN\n",
       "rain_3h_42_ago             NaN\n",
       "snow_3h_45_ago             NaN\n",
       "rain_3h_45_ago             NaN\n",
       "snow_3h_48_ago             NaN\n",
       "rain_3h_48_ago             NaN\n",
       "Name: target_temperature, dtype: float64"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_data['target_temperature'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 879 entries, 48 to 926\n",
      "Columns: 359 entries, pressure to wind_degree_48_ago\n",
      "dtypes: float64(312), int64(47)\n",
      "memory usage: 2.4 MB\n"
     ]
    }
   ],
   "source": [
    "sf_data5.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_label = sf_data5['target_temperature']\n",
    "sf_data6 = sf_data5.drop(['target_temperature'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "\n",
    "        return X[self.attribute_names].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weather_main_columns = list(set(weather_main_columns) - set(['Snow', 'Smoke', 'Squall']))\n",
    "category_columns = []\n",
    "for i in range(1, 12):\n",
    "    category_columns.append('month_{}'.format(i))\n",
    "for i in hour_diffs:\n",
    "    for column in set(weather_main_columns):\n",
    "        category_columns.append('{}_{}_ago'.format(column, i))\n",
    "category_columns += weather_main_columns\n",
    "numercial_columns = list(set(sf_data6) - set(category_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_pipeline = Pipeline([\n",
    "    ('selector', DataFrameSelector(category_columns))\n",
    "])\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('selector', DataFrameSelector(numercial_columns)),\n",
    "    ('scalar', StandardScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "full_pipeline = FeatureUnion(transformer_list=[\n",
    "    (\"num_pipeline\", num_pipeline),\n",
    "    (\"cat_pipeline\", cat_pipeline)\n",
    "    #('pca', PCA())\n",
    "    \n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_train = full_pipeline.fit_transform(sf_data6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -1.32057515e+00,   7.68536019e-01,  -8.56203799e-01, ...,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "       [ -1.32057515e+00,   7.68536019e-01,   1.46419533e+00, ...,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "       [ -1.32057515e+00,  -1.69009196e+00,  -1.31990849e-03, ...,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "       ..., \n",
       "       [  1.39598360e+00,   7.68536019e-01,  -4.89824989e-01, ...,\n",
       "          0.00000000e+00,   0.00000000e+00,   1.00000000e+00],\n",
       "       [  1.39598360e+00,   8.56344161e-01,   5.97432265e-02, ...,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "       [ -1.04891927e+00,   7.68536019e-01,   3.65058902e-01, ...,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00]])"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sf_train[0])\n",
    "sf_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDRegressor(alpha=0.001, average=False, epsilon=0.1, eta0=0.01,\n",
       "       fit_intercept=True, l1_ratio=0.15, learning_rate='invscaling',\n",
       "       loss='squared_loss', n_iter=5, penalty='l1', power_t=0.25,\n",
       "       random_state=42, shuffle=True, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "sgd_clf = SGDRegressor(random_state=42, eta0=0.01, alpha=0.001, penalty='l1')\n",
    "sgd_clf.partial_fit(sf_train, y_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 270.25062764])"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_clf.predict(sf_train[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "926    286.47\n",
       "Name: target_temperature, dtype: float64"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_label[-1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
